train:
    data:
        train: ./data/train.jsonl  # relative path from cwd() to train model.
        val: ./data/val.jsonl  # relative path from cwd() to validate model.
    nbatch: 32  # batch size to train model.
    niter: 3  # number of epoch to train model
    scheduler:  #  see https://huggingface.co/transformers/main_classes/optimizer_schedules.html#schedules
        class: transformers.optimization.get_linear_schedule_with_warmup
        params:
            num_warmup_steps: 0
            num_training_steps: 3

model:
    task: multilabel_textcat
    lang:
        name: ja
        optimizer:  # https://huggingface.co/transformers/main_classes/optimizer_schedules.html#
            class: transformers.optimization.AdamW
            params:
                lr: 1e-4
    labels: ./data/intents.json # relative path from cwd() to speficy intent labels to train model.
    pretrained: cl-tohoku/bert-base-japanese-char-whole-word-masking #  see https://huggingface.co/transformers/pretrained_models.html

hydra:
    run:
        dir: ./models/intent/  # relative directory path from cwd() to output result files.
